---
title: Understanding Alerts
owner: Healthwatch
---

This topic describes the types of alerts you can configure Alertmanager to send.


## <a id='overview'></a> Overview of Alerts




## <a id='om-foundation-alerts'></a> Ops Manager Foundation Alerts

This section describes the alerts you can configure for metrics related to the health of your
Ops Manager foundation.

### <a id='bosh-director-health'></a> BOSHDirectorHealth

The following table describes the alerts you can configure to monitor the health of your BOSH
Director deployment:

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>BOSHDirectorStatus</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The BOSH health SLI test suite fails continuously for a specified amount of time, signaling
      that the BOSH Director is down. As a result, BOSH-managed VMs are less resilient.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>increase(bosh_sli_failures_total{scrape_instance_group="bosh-health-exporter"}[TIME]) > 0</code>,
      where <code>TIME</code> is an amount of time you specify according to the needs of your
      foundation.</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>
      <ol>
        <li>Log in to the <code>bosh-health-exporter</code> VM that your Healthwatch Exporter
          tile deploys by following the procedure in <a href="https://docs.pivotal.io/ops-manager/install/trouble-advanced.html#bosh-ssh">BOSH
          SSH</a> in <em>Advanced Troubleshooting with the BOSH CLI</em> in the Ops Manager
          documentation.</li>
        <li>Review the logs for the <code>bosh-health-exporter</code> VM.</li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [BOSH Health Metric Exporter VM](metrics.html#bosh-health-exporter)
in _Healthwatch Metrics_.

### <a id='cert-expiration'></a> CertificateExpiration

The following table describes the alerts you can configure to monitor when Ops Manager certificates
are due to expire:

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>ExpiringCertificate</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>At least one certificate for your foundation is due to expire within a specified number
      of days.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>ssl_certificate_expiry_seconds &#60; TIME</code>, where <code>TIME</code> is
      the time in seconds that you want to monitor for expiring certificates. For example,
      if you want to receive an alert when a certificate is due to expire within the next 30
      days, replace <code>TIME</code> with <code>2592000</code>, which is the number of seconds
      in a 30-day period.</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>If any certificates for your deployment are due to expire soon, rotate them before
    they expire to avoid downtime for your deployment. To rotate certificates, see <a href="https://docs.pivotal.io/ops-manager/security/pcf-infrastructure/api-cert-rotation.html">Overview
    of Certificate Rotation</a> in the Ops Manager documentation.
    </td>
  </tr>
</table>

For more information, see [Certificate Expiration Metric Exporter VM](metrics.html#cert-expiration-exporter)
in _Healthwatch Metrics_ and [Monitoring Certificate Expiration](configuring/optional-config/certificate-monitoring.html).

### <a id='om-health'></a> OpsManagerHealth

The following table describes the alerts you can configure to monitor the health of your Ops
Manager foundation [or is it the availability of the Ops Manager Installation Dashboard?]:

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>OpsManagerHealth</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The canary test for the Ops Manager Installation Dashboard failed. As a result, operators cannot access the Ops Manager Installation Dashboard.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>probe_success{instance="&#60;OPS-MANAGER-URL>"} <= 0</code>, where <code>OPS-MANAGER-URL</code>
      is the fully-qualified domain name (FQDN) of your Ops Manager deployment. A value of
      <code>0</code> indicates that the canary test for the Ops Manager Installation Dashboard
      failed.</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [Prometheus VM](metrics.html#tsdb) in _Healthwatch Metrics_.


## <a id='tas-alerts'></a> TAS for VMs Alerts

This section describes the alerts you can configure for metrics related to the health of your
TAS for VMs, Healthwatch Exporter for TAS for VMs, VMware Tanzu SQL with MySQL for VMs, and
VMware Tanzu RabbitMQ deployments.

### <a id='tas-slos'></a> TanzuApplicationServiceSLOs

The following tables describe the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TanzuSLOCFPushErrorBudget</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The <code>cf_push</code> command is unresponsive. This alert fires when the error budget reaches zero. This commonly occurs when Diego is under-scaled, UAA is unresponsive, or Cloud Controller is unresponsive.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>( (1 - (rate(tas_sli_task_failures_total{task="push"}[28d]) / rate(tas_sli_task_runs_total{task="push"}[28d])
       ) ) - 0.99) * (28 * 24 * 60) <= 0</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>Check the status of Diego, UAA, and Cloud Controller to troubleshoot.</td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TanzuSLOCFPushAvailability</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>This alert fires when the <code>cf_push</code> command has been unresponsive for 10 minutes. This commonly occurs when Diego is under-scaled, UAA is unresponsive, or Cloud Controller is unresponsive.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>rate(tas_sli_task_failures_total{task="push"}[5m:15s]) * 300 > 0</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>Check the status of Diego, UAA, and Cloud Controller to troubleshoot.</td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TanzuSLOCanaryAppErrorBudget</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>This alert fires when your error budget for your Canary URLs is below zero. If your Canary URLs are representative of other running apps, this could indicate that your end users are affected.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>( (avg_over_time(probe_success[28d]) - 0.999) * (28 * 24 * 60) ) <= 0</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>Check to see if your canary apps are running. Then check your foundation's networking, capacity, and VM health.
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TanzuSLOCanaryAppAvailability</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>A Canary URL has been unresponsive for at least 5 minutes. If your Canary URL is representative of other running apps, this could indicate that your end users are affected.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>avg_over_time(probe_success[5m]) < 1</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>Check to see if your canary apps are running. Then check your foundation's networking, capacity, and VM health.
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.

### <a id=''></a> TASCLIHealth

The following table describes the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TASCLICommandStatus</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>One or more CLI tests have been failing for at least 10 minutes. App Smoke Tests run every 5 minutes. When running HA, multiple smoke tests may run in the given 5 minutes. These tests are intended to give operators confidence that developers can successfully interact with and manage apps on the platform.
    Note: smoke tests report a failure if any task (e.g. `push`, `login`) takes more than 5 minutes to complete.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>increase(tas_sli_task_failures_total[10m]) > 0</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>If a failure occurs, attempt to use the failed CLI command in a terminal to see why it is failing.</td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.

### <a id=''></a> TASDiego

The following tables describe the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TASDiegoMemoryUsed</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>Available memory for Diego Cells is running low. You have exceeded 65% of your available Diego Cell memory capacity for ({{ $labels.placement_tag }}) for at least 10 minutes.
    Low memory can prevent app scaling and new deployments. The overall sum of capacity can indicate that you need to scale the platform. It is recommended that you have enough memory available to suffer a possible failure of an entire availability zone (AZ). If following the best practice guidance of three AZs, your % available memory should always be at least 35%.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>label_replace( (sum by (placement_tag) (CapacityTotalMemory) - sum by (placement_tag) (CapacityRemainingMemory) ) / sum by (placement_tag) (CapacityTotalMemory), "placement_tag", "cf", "placement_tag", "") > .65</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>Assign more resources to the Diego Cells or assign more Diego Cells by scaling Diego cells in the Resource Config pane of the TAS for VMs tile.
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TASDiegoDiskUsed</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>Available disk for Diego Cells is running low. You have exceeded 65% of your available Diego Cell disk capacity for ({{ $labels.placement_tag }}) for at least 10 minutes.
    Low disk capacity can prevent app scaling and new deployments. The overall sum of capacity can indicate that you need to scale the platform. It is recommended that you have enough disk available to suffer a possible failure of an entire availability zone (AZ). If following the best practice guidance of three AZs, your % available disk should always be at least 35%.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>label_replace( (sum by (placement_tag) (CapacityTotalDisk) - sum by (placement_tag) (CapacityRemainingDisk) ) / sum by (placement_tag) (CapacityTotalDisk), "placement_tag", "cf", "placement_tag", "") > .65</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>Assign more resources to the Diego Cells or assign more Diego Cells by scaling Diego cells in the Resource Config pane of the TAS for VMs tile.
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.

### <a id=''></a> TASMySQLHealth

The following table describes the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TASMySQLStatus</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The TAS for VMs MySQL database is not responding. The MySQL database is used for persistent data storage by several TAS for VMs components. Note that this is the SQL database used by system components, not the MySQL service used by apps running on the platform.
    TAS for VMs components that use system databases include the Cloud Controller, Diego Brain, Gorouter, and the UAA server.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>_mysql_available <= 0</code></td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>
      <ol>
        <li>Run mysql-diag</li>
        <li>check the MySQL Server logs for errors.</li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.

### <a id=''></a> TASRouter

The following table describes the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TASRouterHealth</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The TAS for VMs Gorouter is down. This prevents users from interacting with apps and services on the platform.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>system_healthy{exported_job="router", origin="bosh-system-metrics-forwarder"} <= 0 OR system_healthy{exported_job="router", origin="system_metrics_agent"} <= 0</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>Review detailed TAS for VMs Gorouter metrics and logs for details on the cause of the error.
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TASRouterCPUUtilization</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The TAS for VMs Gorouter is experiencing average CPU utilization above 80%. High CPU utilization of the Gorouter VMs can increase latency and cause throughput, or requests per second, to level off. It is recommended to keep the CPU utilization within a maximum range of 60-70% for best Gorouter performance.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>system_cpu_user{exported_job="router", origin="bosh-system-metrics-forwarder"} >= 80 OR system_cpu_user{exported_job="router", origin="system_metrics_agent"} >= 80</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>Resolve high utilization by scaling the Gorouters horizontally, or vertically by editing the Router VM in the Resource Config pane of the TAS for VMs tile.
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TASRouterFileDescriptors</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>A TAS for VMs Gorouter job has exceeded 90,000 file descriptors over the past 5 minutes. File descriptors are an indication of an impending issue with the Gorouter. Each incoming request to the Gorouter consumes two file descriptors. Without the proper mitigations, an unresponsive app could eventually exhaust the file descriptors in the Gorouter, starving routes from other apps running on TAS for VMs.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>file_descriptors >= 90000</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>Troubleshooting steps:
    (1) Identify which apps are requesting excessive connections and resolve the impacting issues with these apps.
    (2) If above recommended mitigations have not already been taken, do so.
    (3) Consider adding more Gorouter VM resources to increase total available file descriptors.
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.

### <a id=''></a> TASUAA

The following table describes the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TASUAAHealth</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>A UAA VM has been unhealthy for 10 minutes. The Tanzu Application Service UAA on index ({{ $labels.index }}) has been unhealthy for 10 minutes.
    If UAA is down, developers and operators cannot authenticate to access the platform.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>system_healthy{exported_job="uaa", origin="bosh-system-metrics-forwarder"} <= 0 OR system_healthy{exported_job="uaa", origin="system_metrics_agent"} <= 0</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>
      <ol>
        <li>Scale the UAA VMs in BOSH</li>
        <li>See the [UAA Documentation](https://docs.run.pivotal.io/concepts/architecture/uaa.html) for more details and troubleshooting ideas.</li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.

### <a id=''></a> TASUsageService

The following tables describe the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TASUsageServiceEventProcessingLag</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>Usage Service has failed to fetch Events from Cloud Controller (CAPI) for the last 48 hours for the deployment ({{ $labels.deployment }}.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>sum(usage_service_app_usage_event_cc_lag_seconds) by (deployment) >= 172800</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>- Check CAPI - Try `cf curl /v2/app_usage_events`. The response should be 200 with recent events as the payload.
    - Check UAA - Make sure the Usage Service can authenticate with CAPI.
    - Check the network settings.

    * If the Usage Service fails to fetch events for 7 or more days, reach out to support.
    **Data loss can occur if the Usage Service fails to fetch events for more than 29 days. **
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>TASUsageServiceEventFetchingStatus</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>Usage Service event fetching is failing for the deployment ({{ $labels.deployment }}. Typically, this means the Usage Service is healthy, but CAPI is not returning the information that is being requested. Historically, this has happened either due to network failures, or the UAA component not authenticating the Usage Service app successfully.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>sum(usage_service_app_usage_event_fetcher_job_exit_code) by (deployment) >= 1</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>Troubleshooting steps:
    - Check to see if you are able to query CAPI for /v2/app_usage_events and /v2/service_usage_events using the `cf curl` command. A failure would indicate there is a problem outside of the Usage Service app affecting the health of the foundation.
    - Check to see if UAA is working correctly.

    * If the Usage Service Event Fetching is failing for more than 7 days, reach out to support immediately.
    **Data loss can occur if Event Fetching fails for more than 29 days.**
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.

### <a id=''></a> HealthwatchTASSLOs

The following tables describe the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>HealthwatchTASFunctionalExporter</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The TAS for VMs SLI metric exporter VM has been down for 10 minutes.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>service_up{service="pas-sli-exporter"} < 1</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>HealthwatchTASCounterExporter</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The counter metric exporter VM has been down for 10 minutes.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>service_up{service="pas-exporter-counter"} < 1</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>HealthwatchTASGaugeExporter</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The gauge metric exporter VM has been down for 10 minutes.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>service_up{service="pas-exporter-gauge"} < 1</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>HealthwatchTASTimerExporter</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The timer metric exporter VM has been down for 10 minutes.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>service_up{service="pas-exporter-timer"} < 1</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.

### <a id=''></a> MySQLHealth

The following table describes the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>MySQLSingleNodeAndMultiSiteClusterHealth</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>One or more MySQL Single node or Multi-site cluster instances have been unhealthy for at least 10 minutes. This may have an impact on apps connected to those databases.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>avg by (deployment) (_p_mysql_available unless on (index, deployment) _p_mysql_galera_wsrep_ready unless on (index, deployment) _p_mysql_follower_is_follower * on (index, deployment) (_p_mysql_system_persistent_disk_used_percent < bool 30) * on (index, deployment) (_p_mysql_system_ephemeral_disk_used_percent < bool 95) * on (index, deployment) (_p_mysql_performance_cpu_utilization_percent < bool 90) * on (index, deployment) system_healthy{exported_job=~".*mysql.*", deployment=~"service-instance.*", origin="bosh-system-metrics-forwarder"}) < 1</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>- Check the MySQL Server logs for errors
    - Check disk capacity
    - Check CPU utilization
    - See more at https://docs.pivotal.io/p-mysql/monitor.html and https://docs.pivotal.io/p-mysql/troubleshoot.html
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>MySQLLeaderFollowerClusterHealth</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>One or more MySQL Leader-Follower cluster instances have been unhealthy for at least 10 minutes. This may have an impact on apps connected to those databases.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>avg by (deployment) (_p_mysql_follower_slave_io_running * on (index, deployment) _p_mysql_follower_slave_sql_running * on (index, deployment) _p_mysql_available * on (index, deployment) (_p_mysql_system_persistent_disk_used_percent < bool 30) * on (index, deployment) (_p_mysql_system_ephemeral_disk_used_percent < bool 95) * on (index, deployment) (_p_mysql_performance_cpu_utilization_percent < bool 90) * on (index, deployment) system_healthy{exported_job=~".*mysql.*", deployment=~"service-instance.*", origin="bosh-system-metrics-forwarder"}) <= .5</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>- Check the MySQL Server logs for errors
    - Run the inspect errand to see if nodes are correctly configured for replication
    - Check disk capacity
    - Check CPU utilization
    - See more at https://docs.pivotal.io/p-mysql/monitor.html and https://docs.pivotal.io/p-mysql/troubleshoot.html
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>MySQLHighAvailabilityClusterHealth</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>One or more HA MySQL cluster instances have been unhealthy for at least 10 minutes. This may have an impact on apps connected to those databases.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>avg by (deployment) (_p_mysql_galera_wsrep_ready * on (index, deployment) _p_mysql_available * on (index, deployment) (_p_mysql_system_persistent_disk_used_percent < bool 90) * on (index, deployment) (_p_mysql_system_ephemeral_disk_used_percent < bool 95) * on (index, deployment) (_p_mysql_performance_cpu_utilization_percent < bool 90) * on (index, deployment) system_healthy{exported_job=~".*mysql.*", deployment=~"service-instance.*", origin="bosh-system-metrics-forwarder"}) <= .67</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>- Check the MySQL Server logs for errors
    - Run mysql-diag on the mysql-jumpbox instance for the cluster to check the cluster's state
    - Ensure no infrastructure event is affecting intra-cluster communication
    - Check disk capacity
    - Check CPU utilization
    - See more at https://docs.pivotal.io/p-mysql/monitor.html and https://docs.pivotal.io/p-mysql/troubleshoot.html
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.

### <a id=''></a> RabbitMQHealth

The following table describes the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>RabbitMQClusterHealth</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>At least 50% of RabbitMQ nodes for a deployment are down.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>avg by (deployment) (min by (deployment, index) ( (1 - _p_rabbitmq_rabbitmq_system_disk_free_alarm) or (1 - _p_rabbitmq_rabbitmq_system_mem_alarm ) or system_healthy{origin="bosh-system-metrics-forwarder", exported_job=~"rabbitmq-server|rabbitmq-haproxy"} or (_p_rabbitmq_rabbitmq_erlang_reachable_nodes == bool on (deployment) group_left count(system_healthy{origin="bosh-system-metrics-forwarder"}) by (deployment) ) ) ) < .5</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>View the RabbitMQ Details dashboard in order to diagnose the issue for the ({{ $labels.deployment }}) deployment.</td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.


## <a id=''></a> TKGI Alerts

This section describes the alerts you can configure for metrics related to the health of your
TKGI and Healthwatch Exporter for TKGI deployments.

### <a id=''></a> KubernetesClusterMasterNodes

The following table describes the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>KubernetesClusterMasterNodeHealth</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>One or more TKGI clusters are running with unhealthy control plane nodes for at least 10 minutes. This might affect the operator's ability to administer changes to the clusters they oversee.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>avg by (cluster) (min by (cluster, instance) ( label_replace(etcd_server_has_leader{}, "instance", "$1", "instance", "(.*):.*") or label_replace(up{job=~".*-kube-scheduler"}, "instance", "$1", "instance", "(.*):.*") or label_replace(up{job=~".*-kube-controller-manager"}, "instance", "$1", "instance", "(.*):.*") or label_replace(up{job=~".*-kubernetes-apiservers"}, "instance", "$1", "instance", "(.*):.*") ) ) < .35</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>
      <ol>
        <li>Identify which clusters are impacted using Kubernetes Cluster Overview dashboard</li>
        <li>view the Cluster Detail dashboard by clicking on the cluster's name in the `Clusters That Might Need Attention` panel to see if API server, scheduler, controller manager, and etcd are up.</li>
        <li>Identify the corresponding BOSH deployment and investigate the logs for the failing jobs on the control plane VMs.</li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.

### <a id=''></a> KubernetesSLITests

The following table describes the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>KubernetesSLITests</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The TKGI SLI test ({{ $labels.task }}) has been failing for at least 10 minutes. TKGI SLI tests run every 1 minute by default. This setting is configurable in Ops Manager and may have been changed. These tests are intended to give operators confidence that developers can successfully interact with and manage apps in their clusters.
    Note: the SLI tests report a failure if any task (e.g. `login`, `clusters`) takes more than 1 minute to complete.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>increase(tkgi_sli_task_failures_total[2m]) <= 0</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>If a failure occurs, attempt to use the failed Kubernetes CLI command in a terminal to see why it is failing.</td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.

### <a id=''></a> HealthwatchTKGISLOs

The following table describes the alerts you can configure to monitor :

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>HealthwatchTKGIFunctionalExporter</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The TKGI SLI metric exporter VM has been down for 10 minutes.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>service_up{service="pks-sli-exporter"} < 1</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

<table>
  <tr>
    <th width="25%">Metric Name</th>
    <td>HealthwatchTKGISystemMetricsExporter</td>
  </tr>
  <tr>
    <th>Description</th>
    <td>The TKGI system metrics exporter VM has been down for 10 minutes.</td>
  </tr>
  <tr>
    <th>YAML Expression</th>
    <td><code>service_up{service="pks-exporter"} < 1</code>, where <code></code> is</td>
  </tr>
  <tr>
    <th>Recommended Response</th>
    <td>
      <ol>
        <li></li>
        <li></li>
      </ol>
    </td>
  </tr>
</table>

For more information, see [ VM](metrics.html#)
in _Healthwatch Metrics_.
